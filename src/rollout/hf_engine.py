"""
HuggingFace æ¨ç†å¼•æ“
ä½¿ç”¨ transformers è¿›è¡Œæ¨ç†ï¼Œé€‚åˆå°æ˜¾å­˜ç¯å¢ƒ
"""
import torch
from typing import List, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer


class HFInferenceEngine:
    """
    åŸºäº HuggingFace çš„æ¨ç†å¼•æ“
    
    ç‰¹æ€§:
    - ç›´æ¥ç”Ÿæˆ token IDs
    - æ”¯æŒ fp16 èŠ‚çœæ˜¾å­˜
    - é€‚åˆ 4GB æ˜¾å­˜
    """
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
    
    def load(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨ç†æ¨¡å‹: {self.model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        # è®¾ç½® pad_tokenï¼ˆä¿®å¤ attention_mask è­¦å‘Šï¼‰
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map="auto",
            dtype=torch.float16,  # ä¿®å¤: torch_dtype â†’ dtype
            trust_remote_code=True
        )
        self.model.eval()
        
        print("âœ… æ¨ç†æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_tokens(
        self,
        input_ids: List[int],
        max_new_tokens: int = 256,  # é™ä½é»˜è®¤å€¼ï¼ŒåŠ å¿«æ¨ç†
        temperature: float = 0.7,
    ) -> List[int]:
        """
        ç”Ÿæˆå¹¶è¿”å›å®Œæ•´çš„ token åºåˆ—
        
        Args:
            input_ids: è¾“å…¥çš„ token IDs
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆé™ä½å¯åŠ å¿«æ¨ç†ï¼‰
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            å®Œæ•´çš„ token åºåˆ— (input + generated)
        """
        device = next(self.model.parameters()).device
        
        # è½¬ä¸º tensor
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # åˆ›å»º attention_maskï¼ˆä¿®å¤è­¦å‘Šï¼‰
        attention_mask = torch.ones_like(input_tensor)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                input_tensor,
                attention_mask=attention_mask,  # æ·»åŠ  attention_mask
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # è¿”å›å®Œæ•´åºåˆ—
        return outputs[0].tolist()
    
    def unload(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
        print("ğŸ—‘ï¸  å¸è½½æ¨ç†æ¨¡å‹...")
        if self.model is not None:
            del self.model
            self.model = None
        torch.cuda.empty_cache()
        print("âœ… æ˜¾å­˜å·²é‡Šæ”¾")
    
    def __enter__(self):
        self.load()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.unload()
