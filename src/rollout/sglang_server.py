"""
SGLang æœåŠ¡å™¨ç®¡ç† - ç‹¬ç«‹è¿›ç¨‹æ¨¡å¼
æ”¯æŒè¿”å› token IDs ä»¥é¿å… tokenizer ä¸ä¸€è‡´
ä¼˜åŒ–é…ç½®é€‚é… 4GB æ˜¾å­˜
"""
import time
import subprocess
import requests
import os
from typing import List, Dict, Any, Optional


class SGLangServer:
    """SGLang æœåŠ¡å™¨ç®¡ç†å™¨ (ç‹¬ç«‹è¿›ç¨‹æ¨¡å¼)"""
    
    def __init__(
        self, 
        model_path: str,
        tp_size: int = 1,
        port: int = 30000,
        mem_fraction: float = 0.5  # 4GB GPU éœ€è¦æ›´å°‘
    ):
        self.model_path = model_path
        self.tp_size = tp_size
        self.port = port
        self.mem_fraction = mem_fraction
        self.process = None
        self.url = f"http://localhost:{port}"
    
    def start(self):
        """å¯åŠ¨ SGLang æœåŠ¡å™¨ (4GB æ˜¾å­˜ä¼˜åŒ–)"""
        print(f"ğŸš€ å¯åŠ¨ SGLang æœåŠ¡å™¨: {self.model_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA adapter
        if os.path.exists(os.path.join(self.model_path, "adapter_config.json")):
            print("\nâŒ é”™è¯¯: æ£€æµ‹åˆ° LoRA adapter")
            print("SGLang éœ€è¦å®Œæ•´çš„æ¨¡å‹ï¼Œä¸èƒ½ç›´æ¥åŠ è½½ LoRA adapter")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹: python scripts/interactive_test.py --model_path Qwen/Qwen2.5-0.5B")
            print("2. æˆ–ä½¿ç”¨ç®€å•æµ‹è¯•: python scripts/simple_test.py --model_path Qwen/Qwen2.5-0.5B")
            raise RuntimeError("ä¸èƒ½ç›´æ¥åŠ è½½ LoRA adapter")
        
        # 4GB æ˜¾å­˜ä¼˜åŒ–é…ç½®
        cmd = [
            "python", "-m", "sglang.launch_server",
            "--model-path", self.model_path,
            "--port", str(self.port),
            
            # æ ¸å¿ƒæ˜¾å­˜é…ç½®
            "--mem-fraction-static", str(self.mem_fraction),  # é™åˆ¶é™æ€æ˜¾å­˜å ç”¨
            
            # å…³é—­æ˜¾å­˜æ¶ˆè€—å¤§çš„åŠŸèƒ½
            "--disable-cuda-graph",        # å…³é—­ CUDA graph (èŠ‚çœ ~512MB)
            "--disable-radix-cache",       # å…³é—­ radix cache
            
            # é™åˆ¶å¹¶å‘å’Œ token æ•°
            "--chunked-prefill-size", "512",    # å‡å° chunked prefill (é»˜è®¤ 2048)
            "--max-running-requests", "2",       # é™åˆ¶å¹¶å‘è¯·æ±‚æ•°
            "--max-total-tokens", "4096",        # é™åˆ¶æ€» token æ•°
            "--max-prefill-tokens", "2048",      # é™åˆ¶ prefill tokens
            
            # CUDA graph é…ç½® (ä»¥é˜²è¢«å¯ç”¨)
            "--cuda-graph-max-bs", "1",          # é™åˆ¶ batch size
            "--cuda-graph-bs", "1",              # åª capture bs=1
        ]
        
        print(f"   ğŸ”§ 4GB æ˜¾å­˜ä¼˜åŒ–é…ç½®:")
        print(f"      - mem_fraction: {self.mem_fraction}")
        print(f"      - âŒ CUDA graph (èŠ‚çœ ~512MB)")
        print(f"      - âŒ radix cache")
        print(f"      - chunked_prefill_size: 512")
        print(f"      - max_running_requests: 2")
        print(f"      - max_total_tokens: 4096")
        
        self.process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        
        # ç­‰å¾…æœåŠ¡å™¨å°±ç»ª
        self._wait_for_ready()
        print(f"âœ… SGLang æœåŠ¡å™¨å·²å°±ç»ª: {self.url}")
    
    def _wait_for_ready(self, timeout: int = 180):
        """ç­‰å¾…æœåŠ¡å™¨å°±ç»ª (å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºå…³é—­äº† CUDA graph)"""
        print("â³ ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨ (å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ)...")
        start_time = time.time()
        last_log_time = start_time
        
        while time.time() - start_time < timeout:
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if self.process.poll() is not None:
                output = self.process.stdout.read()
                raise RuntimeError(
                    f"SGLang æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ (é€€å‡ºç : {self.process.returncode})\n"
                    f"æœ€å 50 è¡Œè¾“å‡º:\n{output[-2000:]}"
                )
            
            # æ‰“å°è¿›åº¦
            current_time = time.time()
            if current_time - last_log_time > 5:
                elapsed = int(current_time - start_time)
                print(f"   ç­‰å¾…ä¸­... ({elapsed}s)")
                last_log_time = current_time
            
            # æ£€æŸ¥å¥åº·çŠ¶æ€
            try:
                response = requests.get(f"{self.url}/health", timeout=2)
                if response.status_code == 200:
                    return
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(2)
        
        # è¶…æ—¶ï¼Œè¯»å–è¾“å‡º
        output_lines = []
        try:
            for line in self.process.stdout:
                output_lines.append(line)
                if len(output_lines) > 50:
                    output_lines.pop(0)
        except:
            pass
        
        raise RuntimeError(
            f"SGLang æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶ ({timeout}s)\n"
            f"æœ€åè¾“å‡º:\n{''.join(output_lines[-20:])}"
        )
    
    def generate_tokens(
        self, 
        input_ids: List[int], 
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> List[int]:
        """ç”Ÿæˆå¹¶è¿”å›å®Œæ•´çš„ token åºåˆ—"""
        response = requests.post(
            f"{self.url}/generate",
            json={
                "input_ids": input_ids,
                "sampling_params": {
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                }
            },
            timeout=120
        )
        
        if response.status_code != 200:
            raise RuntimeError(f"SGLang ç”Ÿæˆå¤±è´¥: {response.text}")
        
        result = response.json()
        
        if "output_ids" in result:
            return result["output_ids"]
        elif "token_ids" in result:
            return result["token_ids"]
        else:
            raise NotImplementedError(
                "SGLang æœªè¿”å› token IDsï¼Œéœ€è¦è°ƒæ•´ API æˆ–å®ç° fallback"
            )
    
    def generate_text(
        self,
        messages: List[Dict[str, str]],
        max_new_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """åŸºäº messages ç”Ÿæˆæ–‡æœ¬ (OpenAI æ ¼å¼)"""
        response = requests.post(
            f"{self.url}/v1/chat/completions",
            json={
                "model": self.model_path,
                "messages": messages,
                "max_tokens": max_new_tokens,
                "temperature": temperature,
            },
            timeout=120
        )
        
        if response.status_code != 200:
            raise RuntimeError(f"SGLang ç”Ÿæˆå¤±è´¥: {response.text}")
        
        result = response.json()
        return result["choices"][0]["message"]["content"]
    
    def shutdown(self):
        """å…³é—­æœåŠ¡å™¨"""
        if self.process:
            print("ğŸ›‘ å…³é—­ SGLang æœåŠ¡å™¨...")
            self.process.terminate()
            try:
                self.process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.process.kill()
                self.process.wait()
            print("âœ… SGLang æœåŠ¡å™¨å·²å…³é—­")
    
    def __enter__(self):
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()
