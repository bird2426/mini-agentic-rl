"""
SFT è®­ç»ƒå™¨ (HuggingFace)
ä½¿ç”¨ transformers + PEFT (LoRA) + bitsandbytes (4-bit)
"""
import torch
from typing import List, Dict, Any
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


class HFSFTTrainer:
    """
    åŸºäº HuggingFace çš„ SFT è®­ç»ƒå™¨
    
    ç‰¹æ€§:
    1. 4-bit é‡åŒ–
    2. LoRA é€‚é…å™¨
    3. æ ‡å‡†çš„ç›‘ç£å¾®è°ƒ
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.optimizer = None
    
    def load_model(self, model_path: str, use_lora: bool = True):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        # 4-bit é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        if use_lora:
            self.model.gradient_checkpointing_enable()
            self.model = prepare_model_for_kbit_training(self.model)
            
            lora_config = LoraConfig(
                r=self.config.get("lora_r", 8),
                lora_alpha=self.config.get("lora_alpha", 16),
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                lora_dropout=self.config.get("lora_dropout", 0.1),
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)
            print(f"   LoRA å‚æ•°: {self.model.print_trainable_parameters()}")
        
        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(
        self, 
        dataset: List[Dict[str, Any]], 
        num_epochs: int = 3
    ):
        """
        SFT è®­ç»ƒ
        
        Args:
            dataset: æ•°æ®é›†ï¼Œæ¯é¡¹åŒ…å«:
                - messages: List[Dict] (OpenAI æ ¼å¼)
            num_epochs: è®­ç»ƒè½®æ•°
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        print(f"\nğŸ¯ å¼€å§‹ SFT è®­ç»ƒ...")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        
        if self.optimizer is None:
            # Use 8-bit AdamW to save memory
            import bitsandbytes as bnb
            self.optimizer = bnb.optim.AdamW8bit(
                self.model.parameters(),
                lr=self.config.get("learning_rate", 5e-5)
            )
        
        batch_size = self.config.get("batch_size", 1)
        grad_accum_steps = self.config.get("gradient_accumulation_steps", 4)
        
        for epoch in range(num_epochs):
            print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
            
            epoch_loss = 0.0
            num_batches = 0
            
            for i in range(0, len(dataset), batch_size):
                batch = dataset[i:i + batch_size]
                
                # Tokenize
                batch_data = self._prepare_batch(batch)
                
                # å‰å‘ä¼ æ’­
                loss = self._compute_loss(batch_data)
                
                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / grad_accum_steps
                loss.backward()
                
                # æ›´æ–°å‚æ•°
                if (i // batch_size + 1) % grad_accum_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜é¿å…ç¢ç‰‡åŒ–
                
                epoch_loss += loss.item() * grad_accum_steps
                num_batches += 1
                
                if num_batches % 10 == 0:
                    avg_loss = epoch_loss / num_batches
                    print(f"  Batch {num_batches}, Loss: {avg_loss:.4f}")
            
            avg_epoch_loss = epoch_loss / num_batches
            print(f"  Epoch {epoch + 1} å¹³å‡ Loss: {avg_epoch_loss:.4f}")
    
    def _prepare_batch(self, batch: List[Dict]) -> Dict:
        """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
        texts = []
        for item in batch:
            text = self.tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)
        
        encoded = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.config.get("max_length", 1024),  # å‡å°‘é»˜è®¤é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜
            return_tensors="pt"
        )
        
        return {
            "input_ids": encoded["input_ids"],
            "attention_mask": encoded["attention_mask"],
        }
    
    def _compute_loss(self, batch: Dict) -> torch.Tensor:
        """è®¡ç®—æŸå¤±"""
        device = next(self.model.parameters()).device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        
        outputs = self.model(
            input_ids=input_ids[:, :-1],
            attention_mask=attention_mask[:, :-1],
            labels=input_ids[:, 1:],
            use_cache=False
        )
        
        return outputs.loss
    
    def save_model(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹"""
        print("ğŸ—‘ï¸  å¸è½½æ¨¡å‹...")
        del self.model
        self.model = None
        torch.cuda.empty_cache()
        print("âœ… æ˜¾å­˜å·²é‡Šæ”¾")
