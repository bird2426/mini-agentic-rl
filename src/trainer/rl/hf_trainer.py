"""
RL è®­ç»ƒå™¨ (HuggingFace)
ä½¿ç”¨ transformers + PEFT + åº”ç”¨ loss-mask
"""
import torch
import torch.nn as nn
from typing import List, Dict, Any
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


class HFRLTrainer:
    """
    åŸºäº HuggingFace çš„ RL è®­ç»ƒå™¨
    
    ç‰¹æ€§:
    1. 4-bit é‡åŒ–
    2. LoRA é€‚é…å™¨
    3. åº”ç”¨ loss-mask
    4. Advantage-weighted loss
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.optimizer = None
    
    def load_model(self, model_path: str, use_lora: bool = True):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        # 4-bit é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        if use_lora:
            self.model.gradient_checkpointing_enable()
            self.model = prepare_model_for_kbit_training(self.model)
            
            lora_config = LoraConfig(
                r=self.config.get("lora_r", 8),
                lora_alpha=self.config.get("lora_alpha", 16),
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                lora_dropout=self.config.get("lora_dropout", 0.1),
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)
            print(f"   LoRA å‚æ•°: {self.model.print_trainable_parameters()}")
        
        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(
        self, 
        trajectories: List[Dict[str, Any]], 
        num_epochs: int = 1
    ):
        """
        RL è®­ç»ƒ (åº”ç”¨ loss-mask)
        
        Args:
            trajectories: è½¨è¿¹åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
                - tokens: List[int]
                - loss_mask: List[int] (0/1)
                - reward: float
            num_epochs: è®­ç»ƒè½®æ•°
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        print(f"\nğŸ¯ å¼€å§‹ RL è®­ç»ƒ...")
        print(f"   è½¨è¿¹æ•°é‡: {len(trajectories)}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        
        if self.optimizer is None:
            # ä½¿ç”¨ 8-bit AdamW èŠ‚çœæ˜¾å­˜
            import bitsandbytes as bnb
            self.optimizer = bnb.optim.AdamW8bit(
                self.model.parameters(),
                lr=self.config.get("learning_rate", 1e-5)
            )
        
        batch_size = self.config.get("batch_size", 1)
        grad_accum_steps = self.config.get("gradient_accumulation_steps", 4)
        
        for epoch in range(num_epochs):
            print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
            
            epoch_loss = 0.0
            num_batches = 0
            
            for i in range(0, len(trajectories), batch_size):
                batch = trajectories[i:i + batch_size]
                
                # å‡†å¤‡æ•°æ®
                batch_tokens, batch_masks, batch_rewards = self._prepare_batch(batch)
                
                # è®¡ç®— loss (åº”ç”¨ mask + GRPO advantage)
                loss = self._compute_loss_with_mask(
                    batch_tokens,
                    batch_masks,
                    batch_rewards,
                    batch  # ä¼ é€’å®Œæ•´çš„ batch ç”¨äº group-relative advantage
                )
                
                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / grad_accum_steps
                loss.backward()
                
                # æ›´æ–°å‚æ•°
                if (i // batch_size + 1) % grad_accum_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜é¿å…ç¢ç‰‡åŒ–
                
                epoch_loss += loss.item() * grad_accum_steps
                num_batches += 1
                
                if num_batches % 10 == 0:
                    avg_loss = epoch_loss / num_batches
                    print(f"  Batch {num_batches}, Loss: {avg_loss:.4f}")
            
            avg_epoch_loss = epoch_loss / num_batches
            print(f"  Epoch {epoch + 1} å¹³å‡ Loss: {avg_epoch_loss:.4f}")
    
    def _prepare_batch(self, batch: List[Dict]) -> tuple:
        """å‡†å¤‡ RL æ‰¹æ¬¡æ•°æ®"""
        # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜
        max_len = min(1024, max(len(traj["tokens"]) for traj in batch))
        
        batch_tokens = []
        batch_masks = []
        batch_rewards = []
        
        for traj in batch:
            tokens = traj["tokens"]
            mask = traj["loss_mask"]
            reward = traj["reward"]
            
            # Truncate
            if len(tokens) > max_len:
                tokens = tokens[:max_len]
                mask = mask[:max_len]
            
            # Padding
            pad_len = max_len - len(tokens)
            tokens_padded = tokens + [self.tokenizer.pad_token_id] * pad_len
            mask_padded = mask + [0] * pad_len
            
            batch_tokens.append(tokens_padded)
            batch_masks.append(mask_padded)
            batch_rewards.append(reward)
        
        # è½¬ä¸º tensor
        tokens_tensor = torch.tensor(batch_tokens, dtype=torch.long)
        masks_tensor = torch.tensor(batch_masks, dtype=torch.float32)
        rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32)
        
        return tokens_tensor, masks_tensor, rewards_tensor
    
    
    def _compute_loss_with_mask(
        self,
        tokens: torch.Tensor,
        loss_masks: torch.Tensor,
        rewards: torch.Tensor,
        trajectories: List[Dict]  # æ–°å¢
    ) -> torch.Tensor:
        """
        è®¡ç®—åº”ç”¨ loss-mask çš„ RL æŸå¤±
        
        æ ¸å¿ƒé€»è¾‘:
        1. è®¡ç®— per-token loss
        2. åº”ç”¨ loss-mask
        3. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡ loss
        4. Advantage-weighted
        """
        device = next(self.model.parameters()).device
        tokens = tokens.to(device)
        loss_masks = loss_masks.to(device)
        rewards = rewards.to(device)
        
        # å‰å‘ä¼ æ’­
        # tokens å·²ç»é€šè¿‡ _prepare_batch è¿›è¡Œäº† max_length é™åˆ¶å’Œ padding
        outputs = self.model(
            input_ids=tokens[:, :-1],
            labels=tokens[:, 1:],
            use_cache=False
        )
        
        # è®¡ç®— per-token loss
        logits = outputs.logits
        vocab_size = logits.size(-1)
        
        shift_labels = tokens[:, 1:].contiguous()
        shift_masks = loss_masks[:, 1:].contiguous()
        
        loss_fct = nn.CrossEntropyLoss(reduction='none')
        token_loss = loss_fct(
            logits.view(-1, vocab_size),
            shift_labels.view(-1)
        ).view_as(shift_labels)
        
        # åº”ç”¨ mask
        masked_loss = token_loss * shift_masks
        
        # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡ loss
        per_sample_loss = (
            masked_loss.sum(dim=1) / 
            shift_masks.sum(dim=1).clamp_min(1)
        )
        
        # Advantage-weighted
        advantages = self._compute_advantages(rewards)
        weighted_loss = (per_sample_loss * advantages).mean()
        
        return weighted_loss
    
    def _compute_advantages(self, rewards: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        if len(rewards) == 1:
            return torch.ones_like(rewards)
        
        advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        return advantages
    
    def save_model(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹"""
        print("ğŸ—‘ï¸  å¸è½½æ¨¡å‹...")
        del self.model
        self.model = None
        torch.cuda.empty_cache()
        print("âœ… æ˜¾å­˜å·²é‡Šæ”¾")
