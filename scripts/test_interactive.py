"""
äº¤äº’å¼æµ‹è¯•è„šæœ¬
æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
"""
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def main():
    parser = argparse.ArgumentParser(description="äº¤äº’å¼æµ‹è¯•")
    parser.add_argument("--model_path", type=str, required=True,
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--max_new_tokens", type=int, default=256,
                       help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="æ¸©åº¦")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("äº¤äº’å¼æµ‹è¯•")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"åŠ è½½ä¸­...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_path,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        device_map="auto",
        dtype=torch.float16,
        trust_remote_code=True
    )
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print("\nè¾“å…¥é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰ï¼š")
    print("=" * 80)
    
    # äº¤äº’å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ’¬ ä½ : ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            # æ„é€ æ¶ˆæ¯
            messages = [
                {"role": "user", "content": user_input}
            ]
            
            # åº”ç”¨ chat template
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=args.max_new_tokens,
                    temperature=args.temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            input_length = inputs['input_ids'].shape[1]
            response = tokenizer.decode(
                outputs[0][input_length:],
                skip_special_tokens=True
            )
            
            print(response)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            continue


if __name__ == "__main__":
    main()
