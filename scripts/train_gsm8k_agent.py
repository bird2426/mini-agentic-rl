import sys
import os
from pathlib import Path

# ä¼˜åŒ–æ˜¾å­˜åˆ†é…
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
from transformers import AutoTokenizer
from src.agents import GSM8KAgent
from src.rollout import RolloutManager
from src.trainer import RLTrainer
from src.datasets import GSM8KDataset


def main():
    parser = argparse.ArgumentParser(description="GSM8K Agent RL è®­ç»ƒ")
    
    # æ¨¡å‹
    parser.add_argument("--model_path", type=str, default="./outputs/Qwen2.5-0.5B/sft",
                       help="SFT æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./outputs/Qwen2.5-0.5B/rl",
                       help="è¾“å‡ºç›®å½•")
    
    # æ•°æ®
    parser.add_argument("--dataset", type=str, default="gsm8k",
                       choices=["gsm8k"])
    parser.add_argument("--samples_per_prompt", type=int, default=4,
                       help="GRPO: æ¯ä¸ªé—®é¢˜é‡‡æ ·å‡ æ¬¡")
    parser.add_argument("--max_new_tokens", type=int, default=256,
                       help="æ¯æ¬¡ç”Ÿæˆçš„æœ€å¤§ token æ•°")
    
    # è®­ç»ƒ
    parser.add_argument("--total_epochs", type=int, default=3,
                       help="è®­ç»ƒè½®æ•°ï¼ˆæ¯è½®: Rollout â†’ Trainingï¼‰")
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    
    # LoRA
    parser.add_argument("--use_lora", action="store_true", default=True)
    parser.add_argument("--lora_r", type=int, default=8)
    
    # è°ƒè¯•
    parser.add_argument("--max_samples", type=int, default=None,
                       help="é™åˆ¶æ ·æœ¬æ•°ï¼ˆé»˜è®¤ None=å…¨éƒ¨ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("GSM8K Agent RL è®­ç»ƒ")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"æ•°æ®: {args.dataset} ({'å…¨éƒ¨' if args.max_samples is None else f'{args.max_samples} æ ·æœ¬'})")
    print(f"GRPO: æ¯ä¸ªé—®é¢˜é‡‡æ · {args.samples_per_prompt} æ¬¡")
    print(f"è®­ç»ƒ: {args.total_epochs} epochs")
    print("=" * 80)
    
    
    # åˆå§‹åŒ–
    agent = GSM8KAgent()  # å†…ç½®å·¥å…·
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    if args.dataset == "gsm8k":
        dataset = GSM8KDataset()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset}")
    
    train_config = {
        "learning_rate": args.learning_rate,
        "batch_size": args.batch_size,
        "gradient_accumulation_steps": args.gradient_accumulation_steps,
        "lora_r": args.lora_r,
        "lora_alpha": args.lora_r * 2,
    }
    
    # è®­ç»ƒå¾ªç¯ï¼ˆä¸ verl ç›¸åŒçš„ç»“æ„ï¼‰
    current_model = args.model_path
    
    for epoch in range(args.total_epochs):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch + 1}/{args.total_epochs}")
        print(f"{'='*80}")
        
        # åŠ è½½æ•°æ®
        train_data = dataset.load("train", max_samples=args.max_samples)
        rl_prompts = [dataset.format_for_rl(item) for item in train_data]
        
        print(f"ğŸ“Š {len(rl_prompts)} ä¸ªé—®é¢˜ Ã— {args.samples_per_prompt} æ¬¡é‡‡æ ·")
        print(f"   = {len(rl_prompts) * args.samples_per_prompt} æ¡è½¨è¿¹")
        
        # Rollout
        print(f"\nğŸ² Rollout")
        with RolloutManager(current_model, agent, tokenizer) as rollout_manager:
            trajectories = rollout_manager.generate_trajectories(
                rl_prompts,
                samples_per_prompt=args.samples_per_prompt,
                max_new_tokens=args.max_new_tokens  # æ§åˆ¶ç”Ÿæˆé•¿åº¦
            )
        
        avg_reward = sum(t["reward"] for t in trajectories) / len(trajectories)
        print(f"âœ… {len(trajectories)} æ¡è½¨è¿¹ï¼Œå¹³å‡ reward: {avg_reward:.3f}")
        
        # Trainingï¼ˆæ¯æ‰¹è½¨è¿¹åªè®­ç»ƒ 1 éï¼‰
        print(f"\nğŸ‹ï¸  Training")
        trainer = RLTrainer(train_config)
        trainer.load_model(current_model, use_lora=args.use_lora)
        trainer.train(trajectories, num_epochs=1)  # å›ºå®šä¸º 1
        
        # ä¿å­˜
        epoch_output_dir = f"{args.output_dir}/epoch_{epoch + 1}"
        trainer.save_model(epoch_output_dir)
        trainer.unload_model()
        
        current_model = epoch_output_dir
        print(f"âœ… Checkpoint: {epoch_output_dir}")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ RL è®­ç»ƒå®Œæˆ!")
    print(f"æ¨¡å‹: {current_model}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
