import sys
import os
from pathlib import Path

# ä¼˜åŒ–æ˜¾å­˜åˆ†é…ï¼Œé¿å…ç¢ç‰‡åŒ–
# PyTorch 2.0+ æ¨èä½¿ç”¨ PYTORCH_ALLOC_CONF (åŸ PYTORCH_CUDA_ALLOC_CONF)
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
from src.datasets import GSM8KDataset
from src.trainer import SFTTrainer


def main():
    parser = argparse.ArgumentParser(description="SFT è®­ç»ƒ")
    
    # æ¨¡å‹
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-0.5B",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./outputs/Qwen2.5-0.5B/sft",
                       help="è¾“å‡ºç›®å½•")
    
    # æ•°æ®
    parser.add_argument("--dataset", type=str, default="gsm8k",
                       choices=["gsm8k"])
    
    # è®­ç»ƒ
    parser.add_argument("--num_epochs", type=int, default=3,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=1,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                       help="æ¢¯åº¦ç´¯ç§¯")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                       help="å­¦ä¹ ç‡")
    
    # LoRA
    parser.add_argument("--use_lora", action="store_true", default=True)
    parser.add_argument("--lora_r", type=int, default=8)
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument("--max_samples", type=int, default=None,
                       help="é™åˆ¶æ ·æœ¬æ•°ç”¨äºå¿«é€Ÿæµ‹è¯• (é»˜è®¤ None=å…¨éƒ¨)")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("SFT è®­ç»ƒ")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"æ•°æ®é›†: {args.dataset} ({'å…¨éƒ¨' if args.max_samples is None else f'{args.max_samples} æ ·æœ¬'})")
    print(f"è®­ç»ƒ: {args.num_epochs} epochs, batch_size={args.batch_size}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“¥ åŠ è½½æ•°æ®é›†...")
    if args.dataset == "gsm8k":
        dataset = GSM8KDataset()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset}")
    
    train_data = dataset.load("train", max_samples=args.max_samples)
    sft_data = [dataset.format_for_sft(item) for item in train_data]
    
    print(f"âœ… åŠ è½½äº† {len(sft_data)} ä¸ªæ ·æœ¬")
    print(f"   æ¯ä¸ª epoch: {len(sft_data)} / {args.batch_size} / {args.gradient_accumulation_steps}")
    print(f"              = {len(sft_data) // (args.batch_size * args.gradient_accumulation_steps)} æ¬¡å‚æ•°æ›´æ–°")
    
    # è®­ç»ƒ
    train_config = {
        "batch_size": args.batch_size,
        "gradient_accumulation_steps": args.gradient_accumulation_steps,
        "learning_rate": args.learning_rate,
        "lora_r": args.lora_r,
        "lora_alpha": args.lora_r * 2,
    }
    
    trainer = SFTTrainer(train_config)
    trainer.load_model(args.model_path, use_lora=args.use_lora)
    trainer.train(sft_data, num_epochs=args.num_epochs)
    trainer.save_model(args.output_dir)
    
    print(f"\n{'='*80}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ¨¡å‹: {args.output_dir}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
